{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0W3vlijxKLd"
   },
   "source": [
    "# HMM и Разметка частей речи\n",
    "\n",
    "Мы не будем реализовывать алгоритмы HMM с нуля, а воспользуемся библиотекой ``hmmlearn`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import MultinomialHMM\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddTB2_igH0vg"
   },
   "source": [
    "## Строим POS-tagger для английского языка\n",
    "- для обучения мы воспользуемся размеченными данными [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus), который можно скачать прямо из библитеки nltk\n",
    "- мы будем использовать universal_tagset из nltk (это не таги Universal Dependencies, о которых мы говорили на лекции)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1589746367152,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhQNy-ts-7hx9PAJ0YHgAIjwD0nfjAtMRdfDrCyQ=s64",
      "userId": "15842932163458061285"
     },
     "user_tz": -180
    },
    "id": "luRAKwiNP1pn",
    "outputId": "384b68e8-207a-4908-d6ca-bf4828ec2591"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/variya/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/variya/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtsiB6X2p7-z"
   },
   "source": [
    "### Вам нужно выполнить следующие шаги, чтобы построить POS-tagger\n",
    "\n",
    "**Вопросы:**\n",
    "\n",
    "1. Возьмите размеченные предложения из  ``nltk.corpus.brown.tagged_sents(tagset='universal')``  \n",
    "\n",
    "    - Используйте ``sklearn.model_selection.train_test_split`` чтобы разделить корпус на  80% training data и 20% testing data.\n",
    "\n",
    "2. Создайте  array ``pos_tags``  содержащий все уникальные POS таги, которые есть в трейн корпусе. Сколько уникальных тагов у вас получилось?\n",
    "\n",
    "3. Найдите 5000 наиболее частов токенов (используйте collections.Counter). \n",
    "    - Перед подсчетом сделайте все буквы в словах маленькими (lowercase)\n",
    "    - Сохраните эти 5000 унаиболее частотных токенов в array ``vocab``. \n",
    "    -  Добавьте токен '[UNK]' в качестве первого элемента ``vocab`` для обозначения всех слов, не вошедших вчастотный словарь =  \"out of vocabulary\")\n",
    "    - Проверьте себя: первые 5 элементов ``vocab`` должны быть \\[\"\\[UNK\\]\", \"the\", \",\", \".\", \"of\", ...\\]\n",
    "\n",
    "4. Используйте ``hmmlearn.hmm.MultinomialHMM``, чтобы создать модель``pos_model`` для разметки частей речи (POS-tagging). \n",
    "\n",
    "  * Установите ``pos_model.startprob_`` используя информацию о доле предложений, начинающихся с каждого из POS тагов из вашего списка уникальных тагов. Например, какая доля предложений начинается с глагола и т.д. \n",
    "     -  Подсказка: это должен быть лист длины ``len(pos_tags)``, сумма вероятностей бытьв начале предложения должна бытьравна 1.\n",
    "  \n",
    "  * Установите ``pos_model.transmat_`` - вероятность перехода от одного тага к другому на основе данных трейн корпуса.\n",
    "      - Подсказка: это должна быть матрица ``(len(pos_tags), len(pos_tags))``, сумма по каждой из строк матрицы должна быть равна 1.\n",
    "  \n",
    "  * Установите ``pos_model.emissionprob_`` - вероятность для каждого токена из ``vocab`` относится к какой-либо части речи. \n",
    "      - Убедитесь, что все токены состоят только из маленьких букв. \n",
    "      - Все токены не из ``vocab`` заменены на  \"\\[UNK\\]\". \n",
    "      - Подсказка: это должна быть матрица ``(len(pos_tags), len(vocab))`` , сумма по каждой из строк матрицы должна быть равна 1.\n",
    "  \n",
    "5. Напишите функцию``get_pos(sentence)``, которая возвращает наиболее вероятную последовательность тегов для некоторого предложения that (``sentence``) \n",
    "    - в этой функции используйте pos_model.decode(...). \n",
    "\n",
    "6. Попробуйте применить вашу модель к нескольким предложениям (слова только измаленьких букв, без пунктуации).\n",
    "    - Обязательно попробуйте для предложений: \"this is a test\", \"saint petersburg is the second-largest city in russia.\", \"i know how to use hmm\". \n",
    "    - Прокомментируйте получившиеся результаты. Похоже на правду?\n",
    "    - Если появились идеи, почему модель ошибается, напишите.\n",
    "\n",
    "**Бонус:** Оцените правильность этой модели на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextHMM(BaseEstimator):\n",
    "\n",
    "    def __init__(self, k, lower=True):\n",
    "        self.pos_tags_ = None\n",
    "        self.vocab_ = None\n",
    "        self.k = k\n",
    "        self.none_token = '[UNK]'\n",
    "        self.lower = lower\n",
    "        self.model = None\n",
    "\n",
    "    def __transform(self, token, lower):\n",
    "        trans_token = token\n",
    "        if lower:\n",
    "            trans_token = token.lower()\n",
    "        if self.vocab_:\n",
    "            if trans_token not in self.vocab_:\n",
    "                return self.none_token\n",
    "        return trans_token\n",
    "\n",
    "    def _set_pos_tags(self, taged_sentences):\n",
    "        sents_pos_tags = [pos_tag[1] for sent in taged_sentences\n",
    "                          for pos_tag in sent]\n",
    "        self.pos_tags_ = list(set(sents_pos_tags))\n",
    "\n",
    "    def _set_vocab(self, tokens):\n",
    "        most_common_tokens = Counter(tokens).most_common(self.k)\n",
    "        self.vocab_ = [token[0] for token in most_common_tokens]\n",
    "        self.vocab_.insert(0, self.none_token)\n",
    "\n",
    "    def _get_startprob(self, taged_sentences):\n",
    "        first_pos_tags = Counter([sent[0][1] for sent in\n",
    "                                 taged_sentences])\n",
    "        total_sents = len(taged_sentences)\n",
    "        startprob = np.array([first_pos_tags[pos_tag] / total_sents\n",
    "                             for pos_tag in self.pos_tags_])\n",
    "        assert abs(1 - startprob.sum()) < 1e-5\n",
    "        return startprob\n",
    "\n",
    "    def _get_transmat(self, taged_sentences):\n",
    "        transmat = np.zeros((len(self.pos_tags_), len(self.pos_tags_)))\n",
    "        for sentence in taged_sentences:\n",
    "            for k in range(len(sentence) - 1):\n",
    "                i = self.pos_tags_.index(sentence[k][1])\n",
    "                j = self.pos_tags_.index(sentence[k + 1][1])\n",
    "                transmat[i, j] += 1\n",
    "        transmat /= transmat.sum(axis=1)[:, np.newaxis]\n",
    "        assert all(np.abs(1 - transmat.sum(axis=1)) < 1e-5)\n",
    "        return transmat\n",
    "\n",
    "    def _get_emissionprob(self, taged_sentences):\n",
    "        pair_count = Counter()\n",
    "        for taged_sentence in taged_sentences:\n",
    "            pair_count.update(taged_sentence)\n",
    "        emissionprob = np.zeros((len(self.pos_tags_), len(self.vocab_)))\n",
    "        for (i, pos_tag) in enumerate(self.pos_tags_):\n",
    "            for (j, token) in enumerate(self.vocab_):\n",
    "                emissionprob[i, j] = pair_count[(token, pos_tag)]\n",
    "        emissionprob /= emissionprob.sum(axis=1)[:, np.newaxis]\n",
    "        assert all(np.abs(1 - emissionprob.sum(axis=1)) < 1e-5)\n",
    "        return emissionprob\n",
    "\n",
    "    def fit(self, taged_sentences, y=None):\n",
    "        self._set_pos_tags(taged_sentences)\n",
    "        tokens = [self.__transform(pos_tag[0], self.lower) for sent in\n",
    "                  taged_sentences for pos_tag in sent]\n",
    "        self._set_vocab(tokens)\n",
    "\n",
    "        transform_pair = lambda pair: (self.__transform(pair[0],\n",
    "                self.lower), pair[1])\n",
    "        transform_sentance = lambda sent: [transform_pair(pair)\n",
    "                for pair in sent]\n",
    "        transformed_taged_sentences = [transform_sentance(sent)\n",
    "                for sent in taged_sentences]\n",
    "\n",
    "        # MultinomialHMM model\n",
    "\n",
    "        self.model = MultinomialHMM(n_components=len(self.pos_tags_))\n",
    "        self.model.emissionprob_ = \\\n",
    "            self._get_emissionprob(transformed_taged_sentences)\n",
    "        self.model.startprob_ = \\\n",
    "            self._get_startprob(transformed_taged_sentences)\n",
    "        self.model.transmat_ = \\\n",
    "            self._get_transmat(transformed_taged_sentences)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def __get_X(self, tokens):\n",
    "        token_code = [self.vocab_.index(self.__transform(token,\n",
    "                      self.lower)) for token in tokens]\n",
    "        return np.array(token_code)[:, np.newaxis]\n",
    "\n",
    "    def get_pos(self, sentence):\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "        X = self.__get_X(sentence.split(' '))\n",
    "        (logprob, seq) = self.model.decode(X)\n",
    "        return [self.pos_tags_[s] for s in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "train_sents, test_sents = train_test_split(sents, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hmm = TextHMM(k=5000, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialHMM(algorithm='viterbi', init_params='ste', n_components=12,\n",
       "        n_iter=10, params='ste', random_state=None, startprob_prior=1.0,\n",
       "        tol=0.01, transmat_prior=1.0, verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hmm.fit(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'VERB', 'DET', 'NOUN']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hmm.get_pos('this is a test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит верно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hmm.get_pos('saint petersburg is the second-largest city in russia.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Название города не распознал, но saint отнес к существительному, в целом выглядит хорошо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'VERB', 'ADV', 'PRT', 'VERB', 'VERB']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hmm.get_pos('i know how to use hmm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь кажется несколько странным сочетания дух подряд идущих глаголов, но на самом деле такое сочетание популярно в обущающих данных и это повлияло на ошибочноую резметку слова hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18459706232631998"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hmm.model.transmat_[text_hmm.pos_tags_.index('VERB'), text_hmm.pos_tags_.index('VERB')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VERB    0.184597\n",
       "ADP     0.169327\n",
       "DET     0.162736\n",
       "ADV     0.103072\n",
       "NOUN    0.097836\n",
       ".       0.080964\n",
       "PRT     0.065352\n",
       "ADJ     0.057652\n",
       "PRON    0.054790\n",
       "CONJ    0.014490\n",
       "NUM     0.009001\n",
       "X       0.000185\n",
       "Name: VERB, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(text_hmm.model.transmat_, columns=text_hmm.pos_tags_, index=text_hmm.pos_tags_).loc['VERB'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'ADP', 'NOUN']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hmm.get_pos('i like cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hmm.get_pos('moscow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_hmm.get_pos('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERB - verbs (all tenses and modes)\n",
    "# NOUN - nouns (common and proper)\n",
    "# PRON - pronouns\n",
    "# ADJ - adjectives\n",
    "# ADV - adverbs\n",
    "# ADP - adpositions (prepositions and postpositions)\n",
    "# CONJ - conjunctions\n",
    "# DET - determiners\n",
    "# NUM - cardinal numbers\n",
    "# PRT - particles or other function words\n",
    "# X - other: foreign words, typos, abbreviations\n",
    "# . - punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = ' '.join([pair[0] for sent in test_sents for pair in sent])\n",
    "test_tags = [pair[1] for sent in test_sents for pair in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_tags = text_hmm.get_pos(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pred_test_tags) == len(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.928\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy {:.3f}'.format(accuracy_score(test_tags, pred_test_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .       1.00      1.00      1.00     29287\n",
      "        ADJ       0.74      0.86      0.80     16449\n",
      "        ADP       0.96      0.97      0.96     28678\n",
      "        ADV       0.90      0.81      0.85     11254\n",
      "       CONJ       0.99      0.99      0.99      7576\n",
      "        DET       0.99      0.99      0.99     27130\n",
      "       NOUN       0.89      0.92      0.90     54933\n",
      "        NUM       0.96      0.76      0.85      2853\n",
      "       PRON       0.98      0.98      0.98     10003\n",
      "        PRT       0.90      0.88      0.89      6088\n",
      "       VERB       0.94      0.88      0.91     36566\n",
      "          X       0.21      0.41      0.27       231\n",
      "\n",
      "avg / total       0.93      0.93      0.93    231048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_tags, pred_test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение присылйте на почту  bdt-mf-ml-nlp-2020-q4@bigdatateam.org  \n",
    "\n",
    "В теме письма укажите: ``HW2:CompLing. ФИО``\n",
    "\n",
    "Пожалуйста, оставьте обратную связь о задании [по ссылке](http://rebrand.ly/mfnlp2020q4_feedback_hw02). Она (при желании) анонимна ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
